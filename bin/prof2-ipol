#! /usr/bin/env python

"""\
%prog <runsdir> [<ipolfile>=ipol.dat] [opts]

Interpolate histo bin values as a function of the parameter space by loading
from the usual data directory structure $datadir/mc/{rundirs}

TODO:
 * Use weight file position matches to exclude some bins, as well as path matching
 * Handle run combination file/string (write a hash of the run list into the ipol filename?)
 * Support asymm error parameterisation
"""

import optparse, os, sys
op = optparse.OptionParser(usage=__doc__)
op.add_option("--pname", "--pfile", dest="PNAME", default="params.dat", help="Name of the params file to be found in each run directory (default: %default)")
op.add_option("--wfile", dest="WFILE", default=None, help="Path to a weight file, used to restrict ipol building to a subset of bins (default: %default)")
op.add_option("--order", dest="ORDER", default=3, type=int, help="Global order of polynomials for interpolation")
op.add_option("--ierr",  dest="IERR", default="median", help="Whether to interpolate MC errors: none, mean, median, symm (default: %default)") #< add rel, asymm
op.add_option("--eorder", dest="ERR_ORDER", default=None, type=int, help="Global order of polynomials for uncertainty interpolation (default: same as from --order)")
op.add_option("--rc", dest="RUNCOMBS", default=None, help="Ru combination file")
# TODO: Add a no-scale option
# TODO: Change to instead (optionally) specify the max number of parallel threads/procs. Use by default if ncores > 1?
op.add_option("-j", dest="MULTI", type=int, default=1, help="Number of threads to use")
op.add_option("--summ",  dest="SUMMARY", default=None, help="Summary description to be written to the ipol output file")
op.add_option("-v", "--debug", dest="DEBUG", action="store_true", default=False, help="Turn on some debug messages")
op.add_option("-q", "--quiet", dest="QUIET", action="store_true", default=False, help="Turn off messages")
opts, args = op.parse_args()

## Get mandatory arguments
if len(args) < 1:
    print "Argument missing... exiting\n\n"
    op.print_usage()
    sys.exit(1)
RUNSDIR = args[0]
IFILE = "ipol.dat"
if len(args) >= 2:
    IFILE = args[1]


## Load the Professor machinery
import professor2 as prof
if not opts.QUIET:
    print prof.logo


## Load MC run histos and params
try:
    if RUNSDIR.endswith(".yaml"):
        try:
            import yaml
        except ImportError, e:
            print "Unable to import YAML:", e
            import sys
            sys.exit(1)
        PARAMS, HISTOS = prof.read_all_rundata_yaml(RUNSDIR)
    else:
        PARAMS, HISTOS = prof.read_all_rundata(RUNSDIR, opts.PNAME)
    RUNS, PARAMNAMES, PARAMSLIST = prof.mk_ipolinputs(PARAMS)
except Exception, e:
    print e
    sys.exit(1)

# TODO: add runcomb file parsing to select a runs subset
# --rc runcombs.dat:4
# would use the 4th line of the runcombs.dat file
if opts.RUNCOMBS is not None:
    f_rc, line = opts.RUNCOMBS.split(":")
    with open(f_rc) as f:
        temp=[l for l in f]
    thisRC = temp[int(line)].split()

    # Filtering and overwriting
    thisRUNS, thisPARAMSLIST = [], []
    for num, r in enumerate(RUNS):
        if r in thisRC:
            thisRUNS.append(r)
            thisPARAMSLIST.append(PARAMSLIST[num])
    RUNS=thisRUNS
    PARAMSLIST=thisPARAMSLIST


## Some useful announcements about the data loaded and the interpolation planned
if not opts.QUIET:
    minnruns =prof.numCoeffs(len(PARAMNAMES), opts.ORDER)
    if (len(RUNS) < minnruns):
        print "Not enough runs for order %i polynomials --- would require %i"%(opts.ORDER, minnruns)
        for i in xrange(1, opts.ORDER):
            if (prof.numCoeffs(len(PARAMNAMES), opts.ORDER -i) <= len(RUNS)):
                print "Try order %i (min %i runs)"%(opts.ORDER -i, prof.numCoeffs(len(PARAMNAMES), opts.ORDER -i))
        import sys
        sys.exit(1)
    else:
        print "Building %dD interpolations in %d params: require at least %d runs" % \
            (opts.ORDER, len(PARAMNAMES), prof.numCoeffs(len(PARAMNAMES), opts.ORDER))
        print "Loaded %d distinct observables from %d runs" % (len(HISTOS), len(RUNS))


## Weight file parsing to select a histos subset
if opts.WFILE:
    matchers = prof.read_pointmatchers(opts.WFILE)
    for hn in HISTOS.keys():
        if not any(m.match_path(hn) for m in matchers.keys()):
            del HISTOS[hn]
        elif opts.DEBUG:
            print "Observable %s passed weight file path filter" % hn
    print "Filtered observables by path, %d remaining" % len(HISTOS)
HNAMES = HISTOS.keys()

## If there's nothing left to interpolate, exit!
if not HNAMES:
    print "No observables remaining... exiting"
    sys.exit(1)


## Robustness tests and cleaning: only retain runs that contain every histo
# TODO: combine with weights histo vetoing -- should we explicitly unload unused run data, or keep it for the next combination to use? Or do we now leave runcombs to the user?
bad, badnum = [], []
for irun, run in enumerate(RUNS):
    for hn in HNAMES:
        if not HISTOS[hn].has_key(run):
            bad.append(run)
            badnum.append(irun)
            break
if bad:
    print "Found %d bad runs in %d total... removing" % (len(bad), len(RUNS))
    goodr, goodp = [], []
    for irun, run in enumerate(RUNS):
        if not irun in badnum:
            goodr.append(run)
            goodp.append(PARAMSLIST[irun])
    RUNS = goodr
    PARAMSLIST = goodp

## If there's nothing left to interpolate, exit!
if not RUNS:
    print "No valid runs remaining... exiting"
    sys.exit(1)


IHISTOS = {}

import zlib

def worker(q, rdict):
    "Function to make bin ipols and store ipol persistency strings for each histo"
    while True:
        if q.empty():
            break
        hn = q.get()
        histos = HISTOS[hn]
        ih = prof.mk_ipolhisto(histos, RUNS, PARAMSLIST, opts.ORDER, opts.IERR, opts.ERR_ORDER)
        del HISTOS[hn] #< pro-actively clear up memory
        s = ""
        for i, ib in enumerate(ih.bins):
            s += "%s#%d %.5e %.5e\n" % (hn, i, ib.xmin, ib.xmax)
            s += "  " + ib.ival.toString("val") + "\n"
            if ib.ierrs:
                s += "  " + ib.ierrs.toString("err") + "\n"
        del ih #< pro-actively clear up memory
        rdict[hn] = zlib.compress(s, 9) #< save some memory
        del s
        del histos


print "\n\nParametrising...\n"
import time, multiprocessing
time1 = time.time()

## A shared memory object is required for coefficient retrieval
from multiprocessing import Manager
manager = Manager()
tempDict = manager.dict()

## The job queue
q = multiprocessing.Queue()
map(lambda x:q.put(x), HNAMES)

## Fire away
workers = [multiprocessing.Process(target=worker, args=(q, tempDict)) for i in range(opts.MULTI)]
map(lambda x:x.start(), workers)
map(lambda x:x.join(),  workers)

## Finally copy the result dictionary into the object itself
for k in tempDict.keys():
    IHISTOS[k] = tempDict[k]

## Timing
time2 = time.time()
print 'Parametrisation took %0.2f s' % ((time2-time1))

## Active memory clean-up
del HISTOS

## Write out meta info
# TODO: Move the format writing into the prof2 Python library
with open(IFILE, "w") as f:
    if opts.SUMMARY is not None:
        f.write("Summary: %s\n" % opts.SUMMARY)
    #f.write("DataDir: %s\n" % os.path.abspath(RUNSDIR))
    f.write("ProfVersion: %s\n" % prof.version())
    f.write("Date: %s\n" % prof.mk_timestamp())
    f.write("DataFormat: binned 1\n") # This tells the reader how to treat the coefficients that follow
    # Format and write out parameter names
    pstring = "ParamNames:"
    for p in PARAMNAMES:
        pstring += " %s" % p
    f.write(pstring + "\n")
    # Dimension (consistency check)
    f.write("Dimension: %i\n" % len(PARAMNAMES))
    # Interpolation validity (hypercube edges)
    minstring = "MinParamVals:"
    for v in prof.mk_minvals(PARAMSLIST):
        minstring += " %f" % v
    f.write(minstring + "\n")
    maxstring = "MaxParamVals:"
    for v in prof.mk_maxvals(PARAMSLIST):
        maxstring += " %f" % v
    f.write(maxstring + "\n")
    f.write("DoParamScaling: 1\n")
    # Number of inputs per bin
    f.write("NumInputs: %i\n" % len(PARAMSLIST))
    s_runs = "Runs:"
    for r in RUNS:
        s_runs +=" %s"%r
    f.write("%s\n"%s_runs)
    f.write("---\n")

## Write out numerical data for all interpolations
s = ""
for hname in sorted(IHISTOS.keys()):
    ih = IHISTOS[hname]
    s += zlib.decompress(ih)

# Open file for write/append
with open(IFILE, "a") as f:
    f.write(s)
print "\nOutput written to %s" % IFILE
