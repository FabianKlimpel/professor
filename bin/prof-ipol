#! /usr/bin/env python

"""\
%prog <runsdir>

Interpolate histo bin values as a function of the parameter space by loading
from the usual data directory structure $datadir/mc/{rundirs}

TODO:
 * Handle weight files/args
 * Handle run combination file/string (write a hash of the run list into the ipol filename?)
 * User choice of MC error interpolation (mean,median,ipol x none,symm,plusminus,full)
"""

import optparse
op = optparse.OptionParser()
# TODO: Remove demo
op.add_option("--pfile", dest="PFILE", default="params.dat", help="Name of the params file to be found in each run directory (default: %default)")
op.add_option("--ierr",  dest="IERR", default="median", help="Whether to interpolate MC errors: none, mean, median symm, or asymm (default: %default)") #< add rel
op.add_option("--order", dest="ORDER", default=3, type=int, help="Global order of polynomials for interpolation")
op.add_option("--summ",  dest="SUMMARY", default=None, help="Summary description of this fit to be written to the ipol output file")
op.add_option("--debug", dest="DEBUG", action="store_true", default=False, help="Turn on some debug messages")
op.add_option("--quiet", dest="QUIET", action="store_true", default=False, help="Turn off messages")
# TODO: Change to instead (optionally) specify the max number of parallel threads/procs
op.add_option("--multi", dest="MULTI", action="store_true", default=False, help="Run in multiprocessing mode")
# TODO: What is this???
op.add_option("--rebin", dest="REBIN", help="Rebin", default=1, type=int)
# TODO: Add weight file parsing to decide which histos (and bin subsets) to interpolate
opts, args = op.parse_args()

## Use the Professor machinery to load the data into standard objects
import professor2 as prof
if not opts.QUIET:
    print prof.logo

## Obtain run data dir path from the command line

DATADIR = args[0]
IFILE="ipol.dat"

import os, glob, sys
if len(args) == 2:
    IFILE=args[1]



# TODO: add runcomb file parsing to select a runs subset
indirs = glob.glob(os.path.join(DATADIR, "*"))
# TODO: The new last two args look out of place... and what does REBIN *do*? Rebinning is a transformation -- should be a different function
PARAMS, HISTOS = prof.load_rundata(indirs, opts.PFILE, opts.DEBUG, rebin=opts.REBIN)
RUNS, PARAMNAMES, PARAMSLIST = prof.mk_ipolinputs(PARAMS)
HNAMES = HISTOS.keys()


## Some useful announcements about the data loaded and the interpolation planned
# TODO: suppress this if QUIET
print "Building %dD interpolations in %d params: require at least %d runs" % \
    (opts.ORDER, len(PARAMNAMES), prof.min_runs(opts.ORDER, len(PARAMNAMES)))
print "Loaded %d distinct observables from %d runs" % (len(HNAMES), len(RUNS))


# TODO: add weight file parsing to select a histos subset


## Robustness tests and cleaning: only retain runs that contain every histo
# TODO: combine with weights histo vetoing -- should we explicitly unload unused run data, or keep it for the next combination to use? Or do we now leave runcombs to the user?
bad, badnum = [], []
for irun, run in enumerate(RUNS):
    for h in HNAMES:
        if not HISTOS[h].has_key(run):
            bad.append(run)
            badnum.append(irun)
            break
if bad:
    print "Found %d bad runs in %d total... removing" % (len(bad), len(RUNS))
    goodr, goodp = [], []
    for irun, run in enumerate(RUNS):
        if not irun in badnum:
            goodr.append(run)
            goodp.append(PARAMSLIST[irun])
    RUNS = goodr
    PARAMSLIST = goodp


## If there's nothing left to interpolate, exit!
if not RUNS:
    print "No valid runs remaining... exiting"
    sys.exit(1)

IHISTOS = {}


# TODO: EXPLAIN THIS!!!!!!
# The worker
#from memory_profiler import profile
#@profile
def worker(q, rdict):
    while True:
        if q.empty():
            break
        hn = q.get()
        histos = HISTOS[hn]
        ih = prof.mk_ipolhisto(histos, RUNS, PARAMSLIST, opts.ORDER, opts.IERR)
        del HISTOS[hn] #< pro-actively clear up memory
        s = ""
        for i, ib in enumerate(ih.bins):
            s += "%s#%d %.5e %.5e\n" % (hn, i, ib.xmin, ib.xmax)
            s += "  " + ib.ival.toString("val") + "\n"
            if ib.ierrs:
                s += "  " + ib.ierrs.toString("err") + "\n"
        del ih #< pro-actively clear up memory
        import zlib
        rdict[hn] = zlib.compress(s, 9) #< save some memory
        del s
        del histos


print "\n\nInterpolating\n"
import time, multiprocessing
num_cores = multiprocessing.cpu_count()
time1 = time.time()
if num_cores > 1 and opts.MULTI:
    ## A shared memory object is required for retrieval coefficients
    from multiprocessing import Manager
    manager = Manager()
    tempDict = manager.dict()

    ## The job queue
    q = multiprocessing.Queue()
    map(lambda x:q.put(x), HNAMES)

    ## Fire away
    workers = [multiprocessing.Process(target=worker, args=(q, tempDict)) for i in range(num_cores)]
    map(lambda x:x.start(), workers)
    map(lambda x:x.join(),  workers)

    ## Finally copy the result dictionary into the object itself
    for k in tempDict.keys():
        IHISTOS[k] = tempDict[k]
else:
    for num, hn in enumerate(HNAMES):
        if opts.DEBUG:
            print "\n\nInterpolating " + hn
        else:
            print "\r%.1f per cent interpolated" % ((float(num+1)/len(HNAMES))*100),
        histos = HISTOS[hn]
        print hn
        IHISTOS[hn] = prof.mk_ipolhisto(histos, RUNS, PARAMSLIST, opts.ORDER, opts.IERR)
        del HISTOS[hn] #< pro-actively clear up memory

time2 = time.time()
print '\nInterpolation took %0.2f s'%((time2-time1))


## Active memory clean-up
del HISTOS


## Write out meta info
# TODO: Move the format writing into the prof2 Python library
with open(IFILE, "w") as f:
    if opts.SUMMARY is not None:
        f.write("Summary: %s\n" % opts.SUMMARY)
    f.write("DataDir: %s\n" % DATADIR) #< TODO: make abs path?
    f.write("ProfVersion: %s\n" % prof.mk_versionstring())
    f.write("Date: %s\n" % prof.mk_timestamp())
    f.write("DataFormat: binned\n") # This tells the reader how to treat the coefficients that follow
    # Format and write out parameter names
    pstring = "ParamNames:"
    for p in PARAMNAMES:
        pstring += " %s" % p
    f.write(pstring + "\n")
    # Dimension (consistency check)
    f.write("Dimension: %i\n" % len(PARAMNAMES))
    # Interpolation validity (hypercube edges)
    minstring = "MinParamVals:"
    for v in prof.mk_minvals(PARAMSLIST):
        minstring += " %f" % v
    f.write(minstring + "\n")
    maxstring = "MaxParamVals:"
    for v in prof.mk_maxvals(PARAMSLIST):
        maxstring += " %f" % v
    f.write(maxstring + "\n")
    # Number of inputs per bin
    f.write("NumInputs: %i\n" % len(PARAMSLIST))
    f.write("---\n")


## Write out interpolation persistency info
s = ""
for hname in sorted(IHISTOS.keys()):
    ih = IHISTOS[hname]
    if type(ih) is str: ## detect results from multiprocessing
        import zlib
        s += zlib.decompress(ih)
    else:
        for i, ib in enumerate(ih.bins):
            s += "%s#%d %.5e %.5e\n" % (hname, i, ib.xmin, ib.xmax)
            s += "  " + ib.ival.toString("val") + "\n"
            if ib.ierrs:
                s += "  " + ib.ierrs.toString("err") + "\n"
# Open file for write/append
with open(IFILE, "a") as f:
    f.write(s)
print "\nOutput written to %s" % IFILE


# ## Store a record of bad runs?
# TODO: do this is in debug mode only... and write out to a more considered filename / give user control (but let's avoid adding opts for everything)
# with open("bad", "w") as f:
#     for b in bad:
#         f.write("%s\n"%b)
